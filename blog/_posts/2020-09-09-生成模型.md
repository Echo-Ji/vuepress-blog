---
title:          生成模型
date:           2020-09-09
author:         Echo
location:       Beijing 
tags: 
    - 面试
    - 机器学习
    - 数据挖掘
    - 生成模型
    - EM
    - HMM
---

> 面试前整理的一些自己不熟悉的知识点，好想拥有一个硬盘一样的脑袋，可以不忘掉的那种。

生成模型是区别于判别模型而言的一种监督学习模型，它是一种概率模型，比如决策树、朴素贝叶斯、HMM、GMM、条件随机场等。
而非概率模型有感知机、支持向量机、kNN、AdaBoost、K-means、神经网络等。

## EM 算法

EM 算法是一种迭代算法，用于含有隐变量的概率模型的极大似然估计。

其核心是 Q 函数的确定，

> Q函数就是完全数据的对数似然函数 $\log P(Y, Z | \theta)$ 在给定观测数据 Y 和当前参数 $\theta^{(i)}$ 的情况下关于未观测数据 Z 的条件概率分布 $P(Z|Y, \theta^{(i)})$ 的期望。写作 $Q(\theta. \theta^{(i)}) = E_{Z}[\log P(Y, Z | \theta) | Y, \theta^{(i)}]$。

当确定了 Q 函数之后，通过 E 步和 M 步分别求 Q 和 极大化 Q 即可迭代求解参数 $\theta$。

## HMM

隐马尔科夫模型（HMM）是关于时序的概率模型，描述一个由马尔科夫链随机生成不可观测的**状态序列**，再由各个状态随机生成一个观测从而产生**观测序列**的过程。
其三要素为
* 初始状态向量 $\pi$
* 状态转移概率矩阵 $A$
* 观测概率矩阵 $B$

在给定三要素的情况下，HMM 模型表示为 $\lambda = (A, B, \pi)$，它是一个生成模型，表示状态序列和观测序列的联合分布。
HMM 涉及到三个问题
* 概率计算问题：给定 $\lambda$ 和观测序列 $O = (o_1, o_2, \cdots, o_T)$，计算 $P(O | \lambda)$，前向-后向算法可以高效进行概率计算。
* 学习问题：已知观测序列 $O$，估计模型 $\lambda$ 的参数，使得在该模型下观测序列概率 $P(O | \lambda)$ 最大，此时需要 EM 算法的支持。
* 预测问题：已知模型 $\lambda$ 和观测序列 $O$，求使得 $P(I | O)$ 最大的状态序列，维特比算法应用动态规划算法求解路径结构中的最优路径。

