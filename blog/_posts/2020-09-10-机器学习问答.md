---
title:          机器学习问答
date:           2020-09-10
author:         Echo
location:       Beijing 
tags: 
    # - 面试
    - 机器学习
    - 数据挖掘
---


## Q1: 为什么交叉熵可用于代价计算（逻辑回归）？

为了让学习到的模型更贴近真实数据的分布，我们最小化**模型数据分布**与**训练数据分布**之间的 KL 散度，$KL(A||B) = - S(A) + H(A, B)$，而因为训练数据的分布是固定的，因此最小化 $KL(A||B)$ 等价于最小化交叉熵 $H(A, B)$。就逻辑回归而言，似然函数的最大化就是交叉熵的最小化。

## Q2：最小二乘和极大似然。

最小二乘法的几何意义是高维空间中的一个向量在低维子空间的投影，其目的是构造目标函数，使得模型拥有一个优化的目标。同时注意最小二乘法的矩阵形式涉及到矩阵**求逆**。

最小二乘和极大似然不是对立的，最小二乘是从数学形式上来看，极大似然是从概率上来看，并且，最小二乘可以通过高斯噪声假设和极大似然估计推导出来。

最后，区分一下梯度下降，通常机器学习的核心是 model、loss 和优化算法。这里，最小二乘和极大似然都是构造 loss 的方式，但梯度下降是优化 loss 的算法。

谈到求逆，这里也简单提一下。求逆一般针对的是满秩方阵（非奇异矩阵），而求逆的方法有以下四种。

* 高斯消元法：手算 2、3 阶矩阵时使用
* LU 分解：$A = LU$，将 $A$ 分解为上三角阵和下三角阵（可用高斯消元法分解，也可用增广矩阵分解），然后 $A^{-1} = U^{-1}L^{-1}$，此方法由于可以[并行化](https://www.zhihu.com/question/354312069)，在计算机中使用较多
* SVD 分解：也叫奇异值分解，$A = UWV^{T}$，其中 $U, V$ 为正交矩阵（正交矩阵的逆等于其转置），$W$ 为对角阵，因而 $A^{-1} = VW^{-1}U^{T}$（对角阵求逆：对角线元素求倒数）
* QR 分解：$A = QR$，其中 $Q$ 为正交矩阵，$R$ 为上三角阵。

对于一般的奇异矩阵（不满秩方阵）或长方矩阵，其也存在逆矩阵，称为广义逆矩阵。当行满秩或者列满秩时，就会存在单侧逆，当不满秩但也要求逆时，就可以用[伪逆](https://zhuanlan.zhihu.com/p/43494217)，这里一般用 SVD 分解。

## Q3：特征值与奇异值。

### 特征分解

如果一个向量 $v$ 可以表示成方阵 $A$ 的特征向量，那么 $Av = \lambda v$，此处 $\lambda$ 就是 $v$ 对应的特征值，一个矩阵的一组特征向量构成一组正交向量。特征分解就是
$$
A = Q \Sigma Q^{-1}.
$$
其中，$Q$ 就是特征向量组成的正交矩阵，$\Sigma$ 就是特征值组成的对角阵。

其实，一个矩阵就是一个线性变换，而一个变换有很多方向，那么每一个特征向量就代表了一个变换方向（一个线性子空间），而对应的特征值就是这个方向的重要程度。但特征分解存在局限，比如变换的矩阵必须是方阵。

### 奇异值分解

奇异值分解能适用于任意的矩阵，假设 $A$ 是一个 $m \times n$ 的矩阵，
$$
A_{m \times n} = U_{m \times m} \Sigma_{m \times n} V^{T}_{n \times n}.
$$
其中，$U, V$ 为正交矩阵（正交矩阵的逆等于其转置），里面的向量分别称为左、右奇异向量，$\Sigma$ 为奇异值组成的对角阵。

### 特征值与奇异值

若用 $A^{T}A$，则得到一个方阵，可以求其特征值：
$$
(A^{T}A) v_i = \lambda v_i.
$$
这里的 $v_i$ 就是右奇异向量，奇异值 $\sigma_i = \sqrt{\lambda_i}$，左奇异向量 $u_i = \frac{1}{\sigma_i} Av_i$，因而奇异值在一定程度上可以代表特征值。

参考 [1](https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html)。

## Q4：伴随矩阵怎么求？

[三步走](https://www.shuxuele.com/algebra/matrix-inverse-minors-cofactors-adjugate.html)：
* 为每个元素，去掉本行和本列，计算行列式，构成**余子式矩阵**
* 把纵横交错的正负号放在余子式矩阵中，形成**代数余子式矩阵**
* 对代数余子式矩阵求**转置**得到伴随矩阵

## Q5：矩阵求导？

这与标量对标量求导的法则有所差异，因此不能直接套用那套准则，需要通过将矩阵导数与微分建立联系，求解矩阵微分进而解决矩阵求导，下面分别附上[标量对矩阵求导](https://zhuanlan.zhihu.com/p/24709748)法则以及[矩阵对矩阵求导](https://zhuanlan.zhihu.com/p/24863977)法则。

## Q6：Loss 和 Cost？

Loss 是针对单个训练样本而言的，而 Cost 是针对算法的参数而言的（所有样本）。

## Q7：期望风险、经验风险与结构风险。

**期望风险**是模型关于数据的联合分布的期望损失。

**经验风险**是模型关于训练样本集的平均损失。

根据大数定理，在训练样本集足够大的情况下，经验风险趋近于期望风险。

因此可以用经验风险来估计期望风险，但是在数据样本较小的时候，会存在过拟合现象，需要对经验风险进行矫正，这时就要用到**结构风险**了。

## Q8：极大似然估计、最大后验估计与贝叶斯估计。

贝叶斯估计的结果是一个分布，而极大似然估计和最大后验估计都是点估计，此外，最大后验估计相较于极大似然估计引入了先验。

有时在估计某个变量在某点处的概率，为了避免极大似然估计出现概率为零的状况，会采用贝叶斯估计（在随机变量的各个取值的频数上赋予一个正数$\lambda > 0$），得到的结果也是一个数值（点），但并不代表贝叶斯估计是点估计。