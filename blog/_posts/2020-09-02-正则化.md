---
title:          正则化
date:           2020-09-02
author:         Echo
location:       Beijing 
tags: 
    - 面试
    - 机器学习
    - 数据挖掘
    - 正则化
---

## L1、L2 正则化

正则化的来源可以从两个角度考虑：
* 带约束条件的优化求解（拉格朗日乘子法）
* 贝叶斯学派的：最大后验概率

L1 正则可以通过假设权重 w 的先验为 Laplace 分布，由 MAP 导出。
L2 正则可以通过假设权重 w 的先验为 Gaussian 分布，由 MAP 导出。

总的来讲，L1 比 L2 更容易获得 sparse 的 w，L2 比 L1 更容易获得 smooth 的 w。

正则化起作用的原因：
* 使得网络中有用的节点变得更少
* 激活函数在 0 附近趋于线性，而线性的网络很难过拟合

一些参考：

https://www.zhihu.com/question/37096933
https://blog.csdn.net/m0_38045485/article/details/82147817

## Dropout 正则化

给定 keep-prob，对每层的神经元输入 a 进行随机删除，使得训练过程中使用更小的网络。注意在随机删除神经元之后，为了保证原始 a 的期望稳定，需要令 a /= keep-prob 进行放缩。

注意，由于每层的参数量有差异，因此过拟合的可能性也不同，因此可以在不同的层设置不用的 keep-prob，参数多的层设置较小的 keep-prob。

其起作用的原因是：
* 每次迭代都使用更小的网络
* 由于不能依赖任何一个特征，因此将权重扩散，这就会收缩权重，像 L2 一样

计算机视觉的人会使用地更多一些，因为数据集一般比较小，很容易过拟合。但缺点是没有 well-defined 的损失函数来观测训练进度。

### BN

BN 是归一化隐藏单元激活值并加速学习的方式。

其奏效的原因是：

* 归一化到 0 均值、1 方差，可以加速学习

* 减弱了前层参数的作用与后层参数的作用之间的联系，它使得网络每层都可以自己学习，稍稍独立于其他层，这有助于加速整个网络的学习

* 往每个隐藏层的激活值上增加了噪音，具有轻微的正则化效果。

