---
title:          集成学习
date:           2020-09-06
author:         Echo
location:       Beijing 
tags: 
    - 面试
    - 机器学习
    - 数据挖掘
    - 集成学习
---

> 面试前整理的一些自己不熟悉的知识点，好想拥有一个硬盘一样的脑袋，可以不忘掉的那种。

## Bagging 与 Boosting

### Bagging 方法

* Bootstrap 采样生成相互独立的 k 个训练集
* 用这 k 个训练集训练 k 个模型
* 用这 k 个模型投票或平均的结果作为模型的输出

例如 Bagging + 决策树 = 随机森林。

随机森林中要注意两个随机，一是为单棵决策树随机有放回地从大小为 N 的训练集中抽取 N 个样本，二是在决策树的每个节点进行分裂时随机从 M 个属性中抽取 m 个属性（m << M），然后从中选择一个最好的进行分裂。

注意，对于第二点而言，如果某节点选择的分裂属性与父节点相同，则该节点停止分裂，变成子节点。

### Boosting 方法

* 每一轮训练都改变数据的概率分布，提升误分类样本的权重，降低正确分类样本的权重
* 给错误率小的基础模型更大的权重，同时减小错误率高的模型权重
* 对基础模型进行加权组合，给出最后的结果

AdaBoost，GBDT，XGBoost 都是这类方法的例子。

### 对比

两种方法对比如下：

| | Bagging | Boosting |
|:--:|:-----|:--------|
| 数据选择 | 采用 Bootstrap 方法有放回地产生相互独立的训练集 | 训练集不变，但每个样本的权重会根据上一轮的分类结果进行调整 |
| 样本权重 | 每个样本权重一致 | 根据错误率调整权重，错误率大的下一轮学习中的权重高，以便学习器注意到该样本 |
| 预测函数 | 所有模型权重一致 | 每个弱分类器都有相应权重，分类效果好的权重高，反之则低 |
| 并行计算 | 每个弱分类器的训练互不干扰，可以并行 | 每一轮的计算都依赖于上一轮模型的分类结果，不能并行 :no_good: |

通常情况下，用 Bagging 方法得到的结果方差更小，而 Boosting 方法得到的结果偏差更小。为了考虑偏差和方差的权衡，就要尽量降低 Bagging 模型的偏差（构建相对复杂的模型），降低 Boosting 模型的方差（简化弱分类器）。

参考 [1](https://medium.com/@pkqiang49/%E4%B8%80%E6%96%87%E7%9C%8B%E6%87%82%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-%E8%AF%A6%E8%A7%A3-bagging-boosting-%E4%BB%A5%E5%8F%8A%E4%BB%96%E4%BB%AC%E7%9A%84-4-%E7%82%B9%E5%8C%BA%E5%88%AB-6e3c72df05b8)、[2](https://zhuanlan.zhihu.com/p/27689464)。

## Blending 与 Stacking

Blending 与 Stacking 方法都是训练一个总模型来组合其他各个模型，首先需要训练多个模型，然后将各个模型的输出作为总模型的输入，用以得到最后的输出。

<!-- 理论上，只要采取合适的模型组合策略，Stacking 就可以表示前两种方法。但实际中，通常采用逻辑回归作为最后的总模型。 -->

Stacking 和 Blending 最大的区别貌似体现在数据划分上，数据划分的不同导致训练过程也有所区别，对比如下。

|      | Blending | Stacking |
|:----:|:--------|:---------|
| 初级，训练 | 把训练数据划分为初级训练集和初级验证集，然后通过初级训练集训练 m 个初级模型，用初级验证集生成次级验证集，次级验证集的大小为（初级验证集的样本数，m） | K 折交叉验证划分训练数据集，初级模型每次生成 1/K 的次级特征，K 次生成一个完整的次级特征，分别用 m 个模型生成次级训练集，其大小为（训练集的样本数，m）
| 初级，测试 | 通过初级模型用初级测试集生成次级测试集 | 通过初级模型用初级测试集生成次级测试集，取 K 次的平均结果作为最后的次级测试集 |
| 次级，训练 | 次级验证集用于训练次级融合模型 | 次级训练集用于训练次级融合模型 |
| 次级，测试 | 次级测试集用于测试 | 次级测试集用于测试 |

相较于 Stacking，Blending 的操作更为简单，但数据利用率也较低。

参考 [1](https://blog.csdn.net/weixin_43467711/article/details/100749340)、[2](https://blog.csdn.net/weixin_43467711/article/details/105258441)。
