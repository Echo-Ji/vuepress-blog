---
title:          集成学习
date:           2020-09-06
author:         Echo
location:       Beijing 
tags: 
    - 面试
    - 机器学习
    - 数据挖掘
    - 集成学习
---

## Bagging 与 Boosting

Bagging 方法：

* Bootstrap 采样生成相互独立的 k 个训练集
* 用这 k 个训练集训练 k 个模型
* 用这 k 个模型投票或平均的结果作为模型的输出

例如 Bagging + 决策树 = 随机森林。

Boosting 方法：

* 每一轮训练都改变数据的概率分布，提升误分类样本的权重，降低正确分类样本的权重
* 给错误率小的基础模型更大的权重，同时减小错误率高的模型权重
* 对基础模型进行加权组合，给出最后的结果

两种方法对比：

| | Bagging | Boosting |
|:--:|:-----|:--------|
| 数据选择 | 采用 Bootstrap 方法有放回地产生相互独立的训练集 | 训练集不变，但每个样本的权重会根据上一轮的分类结果进行调整 |
| 样本权重 | 每个样本权重一致 | 根据错误率调整权重，错误率大的下一轮学习中的权重高，以便学习器注意到该样本 |
| 预测函数 | 所有模型权重一致 | 每个弱分类器都有相应权重，分类效果好的权重高，反之则低 |
| 并行计算 | 每个弱分类器的训练互不干扰，可以并行 | 每一轮的计算都依赖于上一轮模型的分类结果，不能并行 :no_good: |

通常情况下，用 Bagging 方法得到的结果方差更小，而 Boosting 方法得到的结果偏差更小。为了考虑偏差和方差的权衡，就要尽量降低 Bagging 模型的偏差（构建相对复杂的模型），降低 Boosting 模型的方差（简化弱分类器）。

## Stacking

Stacking 方法指的是训练一个总模型来组合其他各个模型，首先需要训练多个模型，然后将各个模型的输出作为总模型的输入，用以得到最后的输出。

理论上，只要采取合适的模型组合策略，Stacking 就可以表示前两种方法。但实际中，通常采用逻辑回归作为最后的总模型。

参考 [1](https://medium.com/@pkqiang49/%E4%B8%80%E6%96%87%E7%9C%8B%E6%87%82%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-%E8%AF%A6%E8%A7%A3-bagging-boosting-%E4%BB%A5%E5%8F%8A%E4%BB%96%E4%BB%AC%E7%9A%84-4-%E7%82%B9%E5%8C%BA%E5%88%AB-6e3c72df05b8)、[2](https://zhuanlan.zhihu.com/p/27689464)。