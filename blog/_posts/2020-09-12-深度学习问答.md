---
title:          深度学习问答
date:           2020-09-12
author:         Echo
location:       Beijing 
tags: 
    - 面试
    - 深度学习
---

## Q1：如何调试超参数？

一般有两种方式：GridSearch 和 RandomSearch。

在选点个数一致的情况下，RandomSearch 会测试更多的超参数，比如对两个超参数选择 25 个点，对于第一个超参数 GridSearch 只能测试 5 个值，而 RandomSearch 也许可以测试 25 个值，这显然更高效，因此个人倾向于 RandomSearch。

## Q2：如何给超参数选择合适的范围？

不同的超参数选择方式不同，但一般分为两种：**线性随机选择**和**指数随机选择**。

**线性随机选择**就是在指定范围内线性生成随机数，写法如下

```Python
# 在 [a, b) 之间线性选择，比如网络层数范围为[2, 5)
para = np.random.rand() * (b - a) + a 
```

这种方式一般适用于神经网络的层数，每层的隐藏单元数等参数。

**指数随机选择**就是对指定范围求对数，然后生成随机数，再指数映射回去，写法如下

```Python
# 对范围 [10^a, 10^b) 取对数，比如学习率范围为 [0.001, 1)
a, b = np.log10(10^a), np.log10(10^b)
# 生成 [a, b) 之间的随机数
r = np.random.rand() * (b - a) + a
# 指数映射回去
para = 10 ^ r
```

这种方式一般适用于学习率，指数加权平均中 $1 - \beta$ 等参数。

## Q3：梯度爆炸/消失是如何产生的？怎么解决？

### 如何产生？
在以梯度下降算法进行反向传播的时候，会运用**链式求导法则**进行更新梯度的计算，而在链式求导的时候就会出现
$$
\frac{\part \mathcal{L}}{\part w_i^{[2]}} = \frac{\part \mathcal{L}}{\part a_i^{[4]}} \cdot \frac{\part a_i^{[4]}}{\part a_i^{[3]}} \cdot \frac{\part a_i^{[3]}}{\part a_i^{[2]}} \cdot \frac{\part a_i^{[2]}}{\part w_i^{[2]}}
$$
这里 $a_i^{[4]} = \hat{y_i}$，而 $ \frac{\part a_i^{[3]}}{\part a_i^{[2]}} = \sigma'(z_i^{[3]})w_i^{[3]}$，可以看到，导数里存在两项：(1) 激活函数的导数，(2) 权重。

当层数累积到一层程度的时候，这两项中的任何一项都会指数变化。比如神经网络有 20 层，
* 权重：当权重大于 1 的时候，假设每一层的权重都为 1.5，那么 1.5^20 会是一个很大的数，就会产生**梯度爆炸**的现象；反之当梯度小于 1，假设为 0.5 的，就会产生**梯度消失**的现象
* 激活函数的导数：当激活函数进入梯度饱和区以后，即导数趋于 0 的时候，就会产生梯度消失的问题

总的来说，产生梯度爆炸/消失的因素有如下三点：
* 链式求导
* 权重
* 激活函数的导数

### 如何解决？

既然知道了原因，那么我们可以分别从不同的角度去解决这个问题。

* 分层训练再 fine-tuning：避免链式求导的累积效应
* 梯度裁剪：当梯度达到某个阈值之后对其进行裁剪强制限制在某个范围内，防止梯度爆炸
* 正则化方法：避免了参数过大梯度爆炸，也避免了激活函数进入饱和区
* 选择激活函数：Relu 类的激活函数在输入大于 0 的时候梯度恒为 1，避免了激活函数进入梯度饱和区
* Batch-Norm：对每一层的输出进行正则化避免了激活函数进入饱和区，也使得权重不会过分依赖于某个特征（不会过大）
* ResNet 残差结构：shortcut 使得有效网络变短，在一定程度上缓解了梯度爆炸/消失问题
