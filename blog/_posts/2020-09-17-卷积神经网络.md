---
title:          卷积神经网络
date:           2020-09-17
author:         Echo
location:       Beijing 
tags: 
    - 卷积
    - 神经网络
    - 深度学习
---

> 面试前整理的一些自己不熟悉的知识点，好想拥有一个硬盘一样的脑袋，可以不忘掉的那种。

## 卷积操作

### 卷积

卷积核也叫过滤器，根据是否使用 Padding 技术，可以分为 Same 卷积和 Valid 卷积。

* Valid 卷积：卷积后图像大小为 $(n - f + 1) \times (n - f + 1)$，其缺点是每次卷积，图像就会变小，第二个缺点是边缘像素的利用率较低。
* Same 卷积：Padding 之后卷积得到的图像大小为 $(n + 2p - f + 1) \times (n + 2p - f + 1)$，要使得卷积前后大小一致，即 $n + 2p - f + 1 = n$，则需要 $p = (f - 1) / 2$，通常采用 $f$ 为奇数的卷积核。

当选择步幅为 $s$ 时，输出图像的大小为 $\frac{n + 2p - f}{s} + 1$，为了保证其为整数，通常向下取整 $\lfloor \frac{n + 2p - f}{s} \rfloor + 1$，为了保证卷积核走出图像时不进行卷积操作。

### 2d/3d 卷积

* 2d 卷积里有单通道卷积核多通道卷积，其中多通道卷积中，各通道的权重可以有差异。
* 3d 卷积的卷积核在深度上不表现差异。注意区别多通道卷积和 3d 卷积。

## 卷积网络

卷积网络通常包含 3 个层：
* 卷积
* 池化：使用池化层来缩减模型的大小，提高计算速度，同时提高所提取特征的鲁棒性。池化会保持通道数不变，它拥有两个参数：大小 $f$ 和步长 $s$，通常采用最大池化，平均池化很少用。注意，池化层不需要学习，他是一个静态属性。
* 全连接

（为什么使用卷积网络？）相比于全连接网络，卷积网络的两个主要优势在于**权重共享**和**稀疏链接**。

卷积网络的发展：
* 经典网络LeNet-5 -> AlexNet -> VGG16
* ResNet(152)
* Inception

我们可以通过池化来压缩图像的长和宽，但是如何压缩通道呢？ $1 \times 1$的卷积，它可以压缩或保持通道数目不变，甚至可以增加通道数量。

## 一些技巧

### 迁移学习

在计算机视觉中，经常存在的一个问题就是数据集太小，这时就可以考虑迁移学习。

用别人已经训练好的权重（预训练），进行 fine-tune，通常会根据数据量的大小冻结不同的层来训练自己的模型。比如当数据量很小的时候，一般就只训练 softmax 层，前面的层都会冻结，随着数据量的增大，可以有选择地解冻更多的层。

### 数据增强

大部分计算机视觉任务需要大量的数据，所以数据增强是通常使用的一种技巧，用以提高视觉系统的表现。

数据增强的方法有：
* （垂直）镜像对称、随机裁剪，当然旋转、剪切、扭曲、变形都可以，只是因为太复杂了用的比较少
* 彩色转换，针对 RGB 图像，基于某些分布，对不同通道的值进行改变。

### 预测技巧

* 使用多个不同的网络进行集成
* 对同一个网络，在测试时进行 multi-crop

这些技巧虽然可以提升性能，但是都会产生相应的时间消耗，因此用在比赛和论文还 :accept:，但实践中就不是很实用。